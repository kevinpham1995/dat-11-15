{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Lab w/ Keras + Deep Learning\n",
    "\n",
    "Welcome to tonight's lab!  For today we're going to try and re-build what was covered in the previous class, but with you behind the driver's wheel.  \n",
    "\n",
    "The purpose is to allow ourselves to better understand the details of model building with neural networks by forcing yourself to recreate what was already covered.  Everything gets more clear with practice.\n",
    "\n",
    "You can refer to class notes from the previous lab, but it's best to try and force yourself to try and remember what you're supposed to do from memory first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import the necessary modules:  numpy, pandas, tensorflow and keras, and load in the dataset.  You can find it in the `data` folder in the `Unit4` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "url = r\"https://raw.githubusercontent.com/JonathanBechtel/dat-11-15/main/ClassMaterial/Unit4/data/IMDB.csv\"\n",
    "df  = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Data Cleaning.  \n",
    "\n",
    "Now go ahead and do some necessary data prep for our modeling.  Try and take the following steps:\n",
    "\n",
    " - Convert the `sentiment` column to 1 and 0\n",
    " - Remove `<br>` tags\n",
    " - Create training and test sets with an 80 / 20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "from sklearn.model_selection import train_test_split\n",
    "df['review'] = df['review'].str.replace('<br />', '')\n",
    "df['sentiment'] = np.where(df['sentiment'] == 'positive', 1, 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Language Processing\n",
    "\n",
    "We'll now go ahead and convert our word corpus into numeric form that can be used for modeling.  We'll do so with the following steps:\n",
    "\n",
    " - Create a tokenizer with a vocabulary size of 10,000 words\n",
    " - Use it to transform the training and test sets (make sure to transform the training set using values in the test set)\n",
    " - Arrange your samples so that all of them are 300 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# tokenizing to develop word index\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words = 10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train   = tokenizer.texts_to_sequences(X_train)\n",
    "X_test    = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# padding to make everything an even length\n",
    "X_train   = keras.preprocessing.sequence.pad_sequences(X_train, maxlen = 300)\n",
    "X_test    = keras.preprocessing.sequence.pad_sequences(X_test, maxlen = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge check:** Can you make sense out of why your new values for `X_train` and `X_test` appear the way they do?  \n",
    "  - Why are there 0's?\n",
    "  - Can you take your numbers and reverse-engineer them back into their original words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 0's when the review is less than 300 characters long, and you need to insert them in order to bring it up to the required length.\n",
    "\n",
    "You can take each number, and look it up in `tokenizer.word_index` to see what word the number corresponds to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Construct a sequential model\n",
    "\n",
    "Now use the `Sequential` module in keras to build a network with the following layers:\n",
    "\n",
    "- A word embedding with 64 weights for each word in our corpus\n",
    "- A layer to resize the embedding output back into two dimensions\n",
    "- Two dense layers with 64 columns of weights each\n",
    "- A dense layer at the end for the final prediction\n",
    "\n",
    "**Note:** Use the appropriate activation functions where necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "mod = keras.models.Sequential([\n",
    "      keras.layers.Embedding(10000, 64, input_length = 300),\n",
    "      keras.layers.Flatten(),\n",
    "      keras.layers.Dense(64, activation = 'relu'),\n",
    "      keras.layers.Dense(64, activation = 'relu'),\n",
    "      keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge check:**  Can you describe what your activation functions actually do?  Specifically:\n",
    " - Could you write a function in regular python that would recreate their behavior?\n",
    " - Can you explain why each one is useful for where it's being used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "def relu(X):\n",
    "    # clip negative values to 0\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "# sigmoid -- coerces values into a probabiity\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). ReLu activations are typically used in the inner layers of a neural network, and they are useful here because they allow neural networks to train faster because their gradients are easy to calculate.  Stylistically, they are helpful because they introduce a very small amount of non-linearity, thus allowing the model to update its weights gradually, without missing an optimum.\n",
    "\n",
    "2).  Sigmoid functions are 'squashing' functions that coerce values into a probability (some float between 0 and 1) and are useful for the final layer because they can mold a model's output into a formal prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Compile your model. \n",
    "\n",
    "This part is a little bit different from what we've done previously.  For a neural network, you have to explicitly tell it what type of loss function to use and (optionally) what metrics to track during training.  \n",
    "\n",
    "Take a moment and read through the keras documentation to find the one that best suits this purpose:  https://keras.io/api/losses/ and use it in the compilation step.  \n",
    "\n",
    "Also specify that your model use the accuracy metric during training.  You can read more about available metrics here:  https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "mod.compile(loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Fit your model.\n",
    "\n",
    "Use the following criteria:\n",
    "\n",
    " - 5 epochs (fitting rounds)\n",
    " - Use 20% of your training data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 23s 23ms/step - loss: 0.3817 - acc: 0.8144 - val_loss: 0.2749 - val_acc: 0.8834\n",
      "Epoch 2/5\n",
      " 539/1000 [===============>..............] - ETA: 10s - loss: 0.1283 - acc: 0.9530"
     ]
    }
   ],
   "source": [
    "# your answer here\n",
    "mod.fit(X_train, y_train.values, epochs = 5, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7:** Make a prediction with your model on a single training sample to validate that it works, and evaluate your test set, and compare with your maximum validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for 1st sample\n",
    "pred = mod.predict(X_train[:1])\n",
    "print(f\"Probability of 1st sample being a positive review: {pred:.2%}\")\n",
    "\n",
    "# test score\n",
    "print(\"Test score is: {mod.evaluate(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
